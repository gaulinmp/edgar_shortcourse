{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#pyEDGAR\" data-toc-modified-id=\"pyEDGAR-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>pyEDGAR</a></span></li><li><span><a href=\"#EDGAR\" data-toc-modified-id=\"EDGAR-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>EDGAR</a></span></li><li><span><a href=\"#EDGAR-Indices\" data-toc-modified-id=\"EDGAR-Indices-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EDGAR Indices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Downloading-indices-(optional)\" data-toc-modified-id=\"Downloading-indices-(optional)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Downloading indices (optional)</a></span></li><li><span><a href=\"#Indices-as-dataframes\" data-toc-modified-id=\"Indices-as-dataframes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Indices as dataframes</a></span></li></ul></li><li><span><a href=\"#EDGAR-Filings\" data-toc-modified-id=\"EDGAR-Filings-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>EDGAR Filings</a></span></li><li><span><a href=\"#Working-with-Filings\" data-toc-modified-id=\"Working-with-Filings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Working with Filings</a></span></li><li><span><a href=\"#Filings-as-Plaintext\" data-toc-modified-id=\"Filings-as-Plaintext-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Filings as Plaintext</a></span></li><li><span><a href=\"#Filings-as-HTML\" data-toc-modified-id=\"Filings-as-HTML-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Filings as HTML</a></span></li><li><span><a href=\"#Homework\" data-toc-modified-id=\"Homework-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Homework</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you know the basics of Python, how to read files and some simple regular expressions.\n",
    "\n",
    "Now let's turn to EDGAR, and all the joys that lie therein.\n",
    "\n",
    "We're going to start by looking at the index of all filings, so we can find which one we want to analyze.\n",
    "Then, we're going to look at specific filings and get comfortable with their format, extracting documents, and pulling info out of HTML.\n",
    "\n",
    "Once again, we approach it from a project/goal driven mindset.\n",
    "With that in mind, here are some things we might want to determine from the document:\n",
    "\n",
    "  1. Word or page count of the document.\n",
    "  1. How many images do firms include in their proxy statements?\n",
    "  1. What are the different sections of a proxy statement?\n",
    "  1. How might we extract a specific section?\n",
    "  1. How might we extract numbers, like CEO's compensation?\n",
    " \n",
    "These are all simple questions with very hard answers.\n",
    "Luckily, there are quite a few libraries or tricks we can employ to try and make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyEDGAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of my PhD, I put together a library I called [pyEDGAR](https://github.com/gaulinmp/pyedgar), which tries to facilitate the most common tasks of interacting with SEC filings on the EDGAR website.\n",
    "\n",
    "You don't have to use pyEDGAR by any means, in fact if you look [here](https://github.com/gaulinmp/pyedgar/blob/master/pyedgar/utilities/edgarweb.py#L130) you can see that it's pretty darn easy to download EDGAR documents with just the [requests](https://2.python-requests.org/en/master/) library.\n",
    "But I'm going to use `pyedgar` because I'm lazy, and don't want to re-write all the parsing code I've already written and packaged up.\n",
    "\n",
    "So, to get pyEDGAR, you can install it in a one-liner:\n",
    "\n",
    "```bash\n",
    "$ pip install git+https://github.com/gaulinmp/pyedgar#egg=pyedgar\n",
    "```\n",
    "\n",
    "Or if you want to pull any future updates:\n",
    "\n",
    "```bash\n",
    "$ cd ~/wherever_you_want_the_library_to_go/\n",
    "$ ggit clone https://github.com/gaulinmp/pyedgar\n",
    "$ cd pyedgar\n",
    "$ pip install -e ./\n",
    "```\n",
    "\n",
    "Either should work out, for now the former is probably easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:45.310535Z",
     "start_time": "2019-09-20T19:08:45.305522Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyedgar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyEDGAR has a config file that it looks for, you probably don't have it.\n",
    "The thing we really care about for now is where it will put files if you want to downolad them locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:46.069625Z",
     "start_time": "2019-09-20T19:08:46.049574Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyedgar import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:46.458887Z",
     "start_time": "2019-09-20T19:08:46.449862Z"
    }
   },
   "outputs": [],
   "source": [
    "# dir() looks at all the functions and variables that are attached to an object.\n",
    "[x for x in dir(config) if x.isupper()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we're interested in is the INDEX_CACHE_ROOT for now, I don't expect you to download all the filings (but you can, with that CACHE_FEED):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:46.926540Z",
     "start_time": "2019-09-20T19:08:46.921526Z"
    }
   },
   "outputs": [],
   "source": [
    "config.INDEX_CACHE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, that will point to a temp file, so if you want to download the indices below, you should make it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:47.374008Z",
     "start_time": "2019-09-20T19:08:47.368994Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.mkdir(config.INDEX_CACHE_ROOT)\n",
    "except FileNotFoundError:\n",
    "    # File not found means /tmp/pyedgar wasn't a folder. So make that first.\n",
    "    os.mkdir(os.path.dirname(config.INDEX_CACHE_ROOT))\n",
    "    os.mkdir(config.INDEX_CACHE_ROOT)\n",
    "except FileExistsError:\n",
    "    print(\"Folder already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T19:08:47.557768Z",
     "start_time": "2019-09-20T19:08:47.553755Z"
    }
   },
   "outputs": [],
   "source": [
    "os.path.exists(config.INDEX_CACHE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR\n",
    "\n",
    "Documentation: [https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm](https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm)\n",
    "\n",
    "EDGAR hosts the public filings of all public companies (and some private) that were submitted since 1995.\n",
    "The filings are associated with one or more firms (identified by CIKs), and each filing has a unique identifier (Accession).\n",
    "\n",
    "Note: One Accession can have multiple CIKs associated with it, so sometimes your UID could just be Accesion, but when matching to Compustat you need the CIK as well.\n",
    "\n",
    "There are two primary things we care about from EDGAR.\n",
    "The first is the filings, obviously.\n",
    "The second is the index of all filings.\n",
    "This is necessary because we need to know what filings exist so we can look them up.\n",
    "Let's start with the index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR Indices\n",
    "\n",
    "EDGAR indices reside at: [https://www.sec.gov/Archives/edgar/full-index/2019/QTR1/](https://www.sec.gov/Archives/edgar/full-index/2019/QTR1/).\n",
    "\n",
    "They contain a list of all filings filed in a given quarter (so it's huge in later years!). \n",
    "We need this list to know what filings we might want to look at, for example all 10-Ks.\n",
    "\n",
    "Here's what the top few lines of those files look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:10:22.465135Z",
     "start_time": "2019-09-21T20:10:22.459527Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "Description:           Master Index of EDGAR Dissemination Feed\n",
    "Last Data Received:    March 31, 2019\n",
    "Comments:              webmaster@sec.gov\n",
    "Anonymous FTP:         ftp://ftp.sec.gov/edgar/\n",
    "Cloud HTTP:            https://www.sec.gov/Archives/\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "CIK|Company Name|Form Type|Date Filed|Filename\n",
    "--------------------------------------------------------------------------------\n",
    "1000045|NICHOLAS FINANCIAL INC|10-Q|2019-02-14|edgar/data/1000045/0001193125-19-039489.txt\n",
    "1000045|NICHOLAS FINANCIAL INC|4|2019-01-15|edgar/data/1000045/0001357521-19-000001.txt\n",
    "1000045|NICHOLAS FINANCIAL INC|4|2019-02-19|edgar/data/1000045/0001357521-19-000002.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get these indexes using EDGARIndex, which just downloads all the quarters since 1995 and puts them into one big table.\n",
    "It also makes separate tables for different form types for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:10:25.993706Z",
     "start_time": "2019-09-21T20:10:25.617717Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyedgar import EDGARIndex\n",
    "idx = EDGARIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at what indices pyEDGAR has found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T20:53:05.775870Z",
     "start_time": "2019-09-20T20:53:05.765845Z"
    }
   },
   "outputs": [],
   "source": [
    "idx.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that makes sense, we haven't downloaded anything yet.\n",
    "How do you download indices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading indices (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:11:17.459772Z",
     "start_time": "2019-09-21T20:11:17.449287Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyedgar.utilities.indices import IndexMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:11:19.218033Z",
     "start_time": "2019-09-21T20:11:19.210912Z"
    }
   },
   "outputs": [],
   "source": [
    "idxm = IndexMaker()\n",
    "idxm._get_index_cache_path('2014Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:11:22.576145Z",
     "start_time": "2019-09-21T20:11:22.571287Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "idxm._tq = tqdm_notebook\n",
    "idxm._downloader._tq = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:16:48.823550Z",
     "start_time": "2019-09-21T20:11:31.225977Z"
    }
   },
   "outputs": [],
   "source": [
    "idxm.extract_indexes(start_year=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices as dataframes\n",
    "\n",
    "Now that we've downloaded some indices, we can take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:11:08.384742Z",
     "start_time": "2019-09-21T20:11:08.368251Z"
    }
   },
   "outputs": [],
   "source": [
    "idx.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what's in the Def-14A, proxy statement filings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:50:59.111052Z",
     "start_time": "2019-09-21T20:50:58.384773Z"
    }
   },
   "outputs": [],
   "source": [
    "d = idx['DEF14A']\n",
    "d[d.name.str.contains(\"Google\")].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this beautiful table?\n",
    "It's a [pandas](https://pandas.pydata.org/) [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html).\n",
    "\n",
    "We'll use a lot more dataframes next class, for now let's just use it to show us `cik` and `accession` and move on to the actual filings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR Filings\n",
    "\n",
    "EDGAR filings are the full text of what a company filed with the SEC.\n",
    "The filing is a weird hybrid of text, HTML, SGML, and attachments which could be anything, like Excel, PDF, Zip, PNG/JPG, etc.\n",
    "\n",
    "What we care about is the general format. It goes like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<SEC-DOCUMENT>\n",
    "    <SEC-HEADER>\n",
    "        <!-- format: HEADER NAME: HEADER VALUE -->\n",
    "        FILER:\n",
    "            COMPANY INFO:\n",
    "                Street Address: etc....\n",
    "    </SEC-HEADER>\n",
    "    <DOCUMENT>\n",
    "        <TYPE>DEFA14A\n",
    "        <SEQUENCE>1\n",
    "        <FILENAME>d935572ddefa14a.htm\n",
    "        <DESCRIPTION>DEFA14A\n",
    "        <TEXT>\n",
    "            <!-- The actual filed document here -->\n",
    "        </TEXT>\n",
    "    </DOCUMENT>\n",
    "    <DOCUMENT>\n",
    "        <TYPE>IMAGE\n",
    "        <SEQUENCE>2\n",
    "        <FILENAME>logo.png\n",
    "        <DESCRIPTION>Logo image file\n",
    "        <TEXT>\n",
    "            <!-- The actual image in 64 bit ascii encoding or something -->\n",
    "        </TEXT>\n",
    "    </DOCUMENT>\n",
    "</SEC-DOCUMENT>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we care about is in those `<TEXT>` tags, or in that `<SEC-HEADER>` tag. \n",
    "\n",
    "This is where you could manually extract that information, or you could let someone else waste their time doing that for you, so you can just jump straight to the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:23.251337Z",
     "start_time": "2019-09-21T20:51:23.242999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the filing\n",
    "from pyedgar import Filing\n",
    "\n",
    "one_def14a = Filing(1288776, '0001193125-05-072803')\n",
    "one_def14a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Filing` object when you first create it doesn't actually read in the filing.\n",
    "That only happens when you actually access the filing's data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:24.822796Z",
     "start_time": "2019-09-21T20:51:24.232393Z"
    }
   },
   "outputs": [],
   "source": [
    "print(one_def14a.full_text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:29.782813Z",
     "start_time": "2019-09-21T20:51:29.775837Z"
    }
   },
   "outputs": [],
   "source": [
    "one_def14a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filing now has `Text:True`, meaning the text is loaded into the filing.\n",
    "But headers and documents haven't been loaded. \n",
    "What does that mean?\n",
    "\n",
    "As we saw above, the structure of a filing has the header section, and then a bunch of documents sequentially listed in the file.\n",
    "The `Filing` object knows about these, but doesn't waste CPU time parsing them until you explicitly ask for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:31.180262Z",
     "start_time": "2019-09-21T20:51:31.171898Z"
    }
   },
   "outputs": [],
   "source": [
    "one_def14a.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:31.835178Z",
     "start_time": "2019-09-21T20:51:31.829044Z"
    }
   },
   "outputs": [],
   "source": [
    "one_def14a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've loaded the headers.\n",
    "\n",
    "There can be multiple filers in an accession, for example Google and Alphabet.\n",
    "They each have an address (which is often the same), and filer information.\n",
    "So how to read in this header?\n",
    "\n",
    "The `Filing` object reads the headers in in two ways: flat and hierarchical.\n",
    "  * **Flat**: All header entries are put in one dictionary, ignoring keys if they already exist.\n",
    "  * **Hierarchical**: Header entries are entered into the dictionary like Flat, but when indentation is found, those indented entries are put in a sub-dictionary.\n",
    "  \n",
    "So for a hierarchical header example:\n",
    "\n",
    "```\n",
    "EFFECTIVENESS DATE:\t\t20150603\n",
    "\n",
    "FILER:\n",
    "\n",
    "\tCOMPANY DATA:\t\n",
    "\t\tCOMPANY CONFORMED NAME:\t\tGoogle Inc.\n",
    "\t\tCENTRAL INDEX KEY:\t\t\t0001288776\n",
    "\n",
    "FILER:\n",
    "\n",
    "\tCOMPANY DATA:\t\n",
    "\t\tCOMPANY CONFORMED NAME:\t\tAlphabet Inc.\n",
    "\t\tCENTRAL INDEX KEY:\t\t\t0001652044\n",
    "```\n",
    "\n",
    "would result in a dictionary looking like:\n",
    "\n",
    "```python\n",
    "{'effectiveness-date':20150603,\n",
    " 'filer': {\n",
    "     'company-data': {\n",
    "         'company-conformed-name': 'Google Inc.',\n",
    "         'central-index-key': '0001288776',\n",
    "     }\n",
    " 'filer_0': {\n",
    "     'company-data': {\n",
    "         'company-conformed-name': 'Alphabet Inc.',\n",
    "         'central-index-key': '0001652044',\n",
    "     }\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our headers are loaded as a dictionary, meaning we can easily extract information from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:51:33.459846Z",
     "start_time": "2019-09-21T20:51:33.452997Z"
    }
   },
   "outputs": [],
   "source": [
    "one_def14a.headers['conformed-submission-type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the filing is the documents.\n",
    "This is the main Def-14A text.\n",
    "\n",
    "We can access it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:56:32.709120Z",
     "start_time": "2019-09-21T20:56:32.705250Z"
    }
   },
   "outputs": [],
   "source": [
    "# one_def14a.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's not actually do that, it's a big file.\n",
    "Instead, we'll use python's pretty-print to format it, and just display the first bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:57:39.349566Z",
     "start_time": "2019-09-21T20:57:39.271308Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(pprint.pformat(one_def14a.documents, width=110)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the documents is a list of documents, each of which is a dictionary, containing the `<DESCRIPTION>`, `<FILENAME>`, and `<TEXT>` (in `full_text`).\n",
    "So to get the text of our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T20:59:07.975359Z",
     "start_time": "2019-09-21T20:59:07.969710Z"
    }
   },
   "outputs": [],
   "source": [
    "print(one_def14a.documents[0]['full_text'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, our filing.\n",
    "Well, the first document of the filing.\n",
    "`Filing`s are always guaranteed to have at least one document (except in error cases), and it's usually the main document (8-K or 10-K, for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Filings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Documentation: [https://www.w3schools.com/html/html_intro.asp](https://www.w3schools.com/html/html_intro.asp)\n",
    "\n",
    "Filings are typically submitted in either HTML form (seen above), or plain text form.\n",
    "The latter is pretty simple to read, but lacks a lot of the contextual clues that we might use to extract data, like bold headings are probably the start of sections.\n",
    "\n",
    "We'll mostly deal with HTML filings here, because that's what most companies file now (text was popular at EDGAR's beginnings, but not so much any more).\n",
    "But our solution to HTML is sometimes just to convert to plain text, so you should be comfortable with both filing types.\n",
    "\n",
    "So let's load up an HTML file and start playing with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:39:58.228621Z",
     "start_time": "2019-09-23T02:39:57.849217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the filing\n",
    "from pyedgar import Filing\n",
    "\n",
    "filing = Filing(1652044, '0001308179-17-000170')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:01.706281Z",
     "start_time": "2019-09-23T02:40:01.624892Z"
    }
   },
   "outputs": [],
   "source": [
    "html = filing.documents[0]['full_text']\n",
    "len(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty long for a proxy document, about 1MB.\n",
    "What does it look like?\n",
    "\n",
    "`Filing`s know that the filings come from EDGAR, so they have urls associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:15.536627Z",
     "start_time": "2019-09-23T02:40:15.528841Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html, HTML\n",
    "# Display it as a link\n",
    "HTML(f\"<a href='{filing.urls[-1]}' target=_blank>{filing.urls[-1]}</a>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:16.652192Z",
     "start_time": "2019-09-23T02:40:16.646536Z"
    }
   },
   "outputs": [],
   "source": [
    "print(html[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be nice to see the actual formatted version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:18.070082Z",
     "start_time": "2019-09-23T02:40:18.048785Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, that document has lots of formatting and stuff, most of which we don't really care about.\n",
    "\n",
    "Parsing HTML documents is its own tutorial, and takes another life time to learn (this is at least the second one needed, after learning Regexes).\n",
    "We'll only touch on the basics here, but as always there's a lot of practice to get comfortable with it.\n",
    "\n",
    "To parse HTML, we could use regular expressions, but that's a bit confounded by the fact that we really want to search the displayed text, not all those html tags like `<div>` or `<div style=\"font-family:times\">`.\n",
    "So we largely have two options:\n",
    "\n",
    "  1. Convert the HTML to plain-text, and then search/parse that plain text like we've done before.\n",
    "  2. Use a library to parse the HTML for us, and use the contextual information we get from the HTML syntax to help us extract data more reliably.\n",
    "  3. Middle-ground: use something like Markdown to convert HTML into plain-text with some context preserved, like headers, bold, italic, etc.\n",
    "  \n",
    "The first way is easier, but sometimes less robust.\n",
    "The second way is harder, but sometimes the only way we can get a reliable extract.\n",
    "The last option, the Markdown approach, is what I used to parse Risk Factor sections, and seemed to work pretty well.\n",
    "\n",
    "There's no one right answer, so your approach should be customized to exactly the data you want to get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filings as Plaintext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to deal with HTML is just to strip all HTML from it, and get the plain-text.\n",
    "There's two ways to do that. \n",
    "\n",
    "First, if we're on linux, and have [w3m](http://w3m.sourceforge.net/) installed, we can use pyedgar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:48:03.909648Z",
     "start_time": "2019-09-23T02:48:03.739768Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyedgar.utilities import htmlparse\n",
    "\n",
    "print(htmlparse.convert_html_to_text(html)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably saw, if you don't have w3m installed, that doesn't work. \n",
    "I only mention it because it's the fastest way to convert html to text that I've found (tested a few different methods), so if you're doing a big project, consider finding a linux server and using it.\n",
    "\n",
    "A second way to convert is using [html2text](http://alir3z4.github.io/html2text/), which is actually solution 3 from above (the middle ground).\n",
    "First, we have to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:37.959067Z",
     "start_time": "2019-09-23T02:40:36.949899Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:41.349574Z",
     "start_time": "2019-09-23T02:40:41.333445Z"
    }
   },
   "outputs": [],
   "source": [
    "import html2text\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:44.606652Z",
     "start_time": "2019-09-23T02:40:44.083944Z"
    }
   },
   "outputs": [],
   "source": [
    "text = h.handle(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:40:45.763168Z",
     "start_time": "2019-09-23T02:40:45.759311Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's not that pretty either.\n",
    "Now you might see why I use w3m (or you can't see, but trust me it's beautifully formatted :).\n",
    "\n",
    "As an explanation of what's happening, the html2text software takes in HTML, and converts it to [Markdown](https://daringfireball.net/projects/markdown/syntax), which is a text file that converts into relatively simple HTML.\n",
    "It uses things like \\*italic\\* for *italic*, or # Heading 1 for a `<h1>` tag.\n",
    "In fact it's what these comments in the notebooks use!\n",
    "\n",
    "This is convenient for things like defining a heading, because if we're looking for `Item 1A: Risk Factor`, we can search for `**Item 1A: Risk Factor**`.\n",
    "\n",
    "As with all scraping work, the best way to figure this out is to try, fail, try again, repeat ad-nauseam until you get your 95% accuracy target.\n",
    "\n",
    "So now we could look for things like **Compensation Discussion and Analysis**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T02:50:33.304153Z",
     "start_time": "2019-09-23T02:50:33.296296Z"
    }
   },
   "outputs": [],
   "source": [
    "'Compensation Discussion and Analysis' in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahoo! \n",
    "Obviously a regex solution would be better, but you learned how to do that in the first lecture, so we don't have to repeat ourselves here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filings as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking at this filing: [https://www.sec.gov/Archives/edgar/data/1652044/000130817917000170/lgoog2017_def14a.htm](https://www.sec.gov/Archives/edgar/data/1652044/000130817917000170/lgoog2017_def14a.htm)\n",
    "\n",
    "To look at a document as HTML, we need (once again) a library.\n",
    "The standard one is called [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "Let's install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:07:44.057169Z",
     "start_time": "2019-09-23T03:07:37.762439Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda install BeautifulSoup4 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:11:49.569374Z",
     "start_time": "2019-09-23T03:11:49.319270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Don't ask why it's bs4, just gotta memorize it\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:11:30.863667Z",
     "start_time": "2019-09-23T03:11:30.704215Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for a crash course in BeautifulSoup (more complete explanation above, in that link).\n",
    "\n",
    "BeautifulSoup (BS from here on out) takes in some HTML, and converts it into a big Python object.\n",
    "That object lets you search for specific tags, extract text, as well as a bunch of functionality we probably don't care about.\n",
    "\n",
    "Let's play around with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.fi # <- put cursor after the i and hit tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:16:06.395732Z",
     "start_time": "2019-09-23T03:16:06.387931Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`soup.find` looks for HTML tags, like the `<p>` tag, which is a paragraph tag.\n",
    "So if we wanted to count all the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:18:14.318740Z",
     "start_time": "2019-09-23T03:18:14.299750Z"
    }
   },
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search for **Compensation Discussion and Analysis** again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:25:57.448658Z",
     "start_time": "2019-09-23T03:25:57.392493Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.findAll(text='Compensation Discussion and Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's neat! What's around those things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:27:25.586932Z",
     "start_time": "2019-09-23T03:27:25.529980Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in soup.findAll(text='Compensation Discussion and Analysis'):\n",
    "    print(x)\n",
    "    break\n",
    "\n",
    "# Because of Python magic, we now have an x to work with\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we ran a loop, and broke out of it. \n",
    "Python leaves all those variables intact, meaning we can now play with x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:28:10.233519Z",
     "start_time": "2019-09-23T03:28:10.226764Z"
    }
   },
   "outputs": [],
   "source": [
    "x.parent # <-- hit tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:28:05.717639Z",
     "start_time": "2019-09-23T03:28:05.710517Z"
    }
   },
   "outputs": [],
   "source": [
    "x.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to get all parents up until body?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:34:55.255624Z",
     "start_time": "2019-09-23T03:34:55.247885Z"
    }
   },
   "outputs": [],
   "source": [
    "newx = x\n",
    "while newx.parent.name != \"body\":\n",
    "    newx = newx.parent\n",
    "\n",
    "newx.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, a table? \n",
    "Let's look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:35:17.664442Z",
     "start_time": "2019-09-23T03:35:17.649134Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(newx.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like at table of contents.\n",
    "Let's look at the second instance we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:37:02.978375Z",
     "start_time": "2019-09-23T03:37:02.921617Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in soup.findAll(text='Compensation Discussion and Analysis'):\n",
    "    newx = x\n",
    "    while newx.parent.name != \"body\":\n",
    "        newx = newx.parent\n",
    "    \n",
    "    print(x, ':', newx.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:37:21.250075Z",
     "start_time": "2019-09-23T03:37:21.243297Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML(newx.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, that looks like a header.\n",
    "I bet it's the start of the CD&A section.\n",
    "Let's look at what comes next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:49:07.732870Z",
     "start_time": "2019-09-23T03:49:07.726730Z"
    }
   },
   "outputs": [],
   "source": [
    "newx.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:48:19.846691Z",
     "start_time": "2019-09-23T03:48:19.805786Z"
    }
   },
   "outputs": [],
   "source": [
    "newx.find_next(attrs=newx.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aww bummer. \n",
    "We want to know why that didn't work, right?\n",
    "\n",
    "Let's brute-force it, by going over to the full filing and searching for that style.\n",
    "\n",
    "  1. Open [this](https://www.sec.gov/Archives/edgar/data/1652044/000130817917000170/lgoog2017_def14a.htm), right click, view source.\n",
    "  1. Search for \"font: 18pt Arial, Helvetica, Sans-Serif; margin: 25pt 0 20pt\"\n",
    "  1. Notice there's only one match. Sad.\n",
    "  1. Think to yourself \"Right above it is font 22pt, right below is font 14pt, this should work\"\n",
    "  1. Notice that the margin: changes in those other ones.\n",
    "  1. Think to yourself \"What if we just try the font part, and omit margins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:55:04.588160Z",
     "start_time": "2019-09-23T03:55:04.540733Z"
    }
   },
   "outputs": [],
   "source": [
    "newx.find_next(attrs={'style': 'font: 18pt Arial, Helvetica, Sans-Serif;'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no. \n",
    "Maybe the problem is that we're searching for an exact match, and we want a partial match.\n",
    "\n",
    "REGEX to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T03:55:58.363883Z",
     "start_time": "2019-09-23T03:55:58.348854Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "newx.find_next(attrs={'style': re.compile('font: 18pt Arial, Helvetica, Sans-Serif;')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand [boom goes the dynamite](https://youtu.be/W45DRy7M1no?t=144).\n",
    "\n",
    "Okay, so we have a CD&A tag beginning, and we have the next tag that follows it.\n",
    "Let's grab everything inbetween:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T04:12:50.575934Z",
     "start_time": "2019-09-23T04:12:50.563828Z"
    }
   },
   "outputs": [],
   "source": [
    "begin_tag = newx\n",
    "end_tag = newx.find_next(attrs={'style': re.compile('font: 18pt Arial, Helvetica, Sans-Serif;')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T04:16:06.787831Z",
     "start_time": "2019-09-23T04:16:06.762227Z"
    }
   },
   "outputs": [],
   "source": [
    "gather_the_html = []\n",
    "for tag in begin_tag.findNextSiblings():\n",
    "    gather_the_html.append(tag.prettify())\n",
    "    if tag == end_tag or tag.find(attrs={'style': re.compile('font: 18pt Arial, Helvetica, Sans-Serif;')}):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often don't know if something works until I run it. \n",
    "I wrote the above, then ran it and hoped.\n",
    "Let's see if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T04:16:08.061911Z",
     "start_time": "2019-09-23T04:16:08.056616Z"
    }
   },
   "outputs": [],
   "source": [
    "len(gather_the_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T04:16:08.903672Z",
     "start_time": "2019-09-23T04:16:08.897273Z"
    }
   },
   "outputs": [],
   "source": [
    "gather_the_html[0], gather_the_html[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay... maybe? \n",
    "Let's display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T04:16:11.882176Z",
     "start_time": "2019-09-23T04:16:11.873666Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML('\\n'.join(gather_the_html).replace('$', '\\$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got lucky here, because there's no nested hierarchy (thanks Google!).\n",
    "That's not always the case, often times companies will wrap each page in a `<div>` tag, so we would have to use `begin_tag.findAllNext()` instead, and then somehow rule out duplicate tags (e.g. because of finding children).\n",
    "\n",
    "As you might have seen, scraping HTML is a bear of a task, and I do it very iteratively.\n",
    "But it lets you do things like we just did, saying find a heading, then find the next heading at the same level.\n",
    "This wouldn't have been possible in plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the filing from above ([this one](https://www.sec.gov/Archives/edgar/data/1652044/000130817917000170/lgoog2017_def14a.htm)), read in the HTML and answer the following questions:\n",
    "\n",
    "\n",
    "\n",
    "  1. How many words are in the filing?\n",
    "  1. How many pages are in the filing?\n",
    "  1. How many images are included in the filing?\n",
    "  1. What are the different sections of the proxy statement? (hint: see the table of contents we found above).\n",
    "  1. What are the top 5 people (by salary) paid at Google?\n",
    "  1. *WITHOUT CODING*: Describe how you would go about extracting this information programatically.\n",
    "     1. What format is this information in?\n",
    "     1. Do you think it's repeatable for not-Google?\n",
    "     1. Would you use HTML or plain text to get this information?\n",
    "     1. Extra credit: Write out some [pseudo-code](https://www.vikingcodeschool.com/software-engineering-basics/what-is-pseudo-coding) for your approach, or just the steps in plain english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "589.797px",
    "left": "1363.31px",
    "top": "135.797px",
    "width": "180.063px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
